{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(\".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    api_key=os.environ[\"GEMINI_API_KEY\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_prompt = \"Explain the concept of prompt engineering in one sentence.\"\n",
    "print(llm.invoke(basic_prompt).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Provide a definition of {topic}, explain its importance, and list three key benefits.\"\n",
    ")\n",
    "\n",
    "chain = structured_prompt | llm # Combine the prompt template with the language model\n",
    "input_variables = {\"topic\": \"prompt engineering\"} # Define the input variables\n",
    "output = chain.invoke(input_variables).content # Invoke the chain with the input variables\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-turn Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain.memory import ChatMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Chain the prompt and the LLM\n",
    "chain = prompt | llm\n",
    "\n",
    "# Create a runnable with message history\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    lambda session_id: ChatMessageHistory(),\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\",\n",
    ")\n",
    "\n",
    "# Example usage\n",
    "conversation_id = \"space_conversation\"\n",
    "messages = []\n",
    "\n",
    "# First query\n",
    "response1 = chain_with_history.invoke(\n",
    "    {\"input\": \"Hi, I'm learning about space. Can you tell me about planets?\"}, \n",
    "    config={\"configurable\": {\"session_id\": conversation_id}}\n",
    ")\n",
    "print(response1.content)\n",
    "\n",
    "# Second query\n",
    "response2 = chain_with_history.invoke(\n",
    "    {\"input\": \"What's the largest planet in our solar system?\"}, \n",
    "    config={\"configurable\": {\"session_id\": conversation_id}}\n",
    ")\n",
    "print(response2.content)\n",
    "\n",
    "# Third query\n",
    "response3 = chain_with_history.invoke(\n",
    "    {\"input\": \"How does its size compare to Earth?\"}, \n",
    "    config={\"configurable\": {\"session_id\": conversation_id}}\n",
    ")\n",
    "print(response3.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Templating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jinja2 import Template\n",
    "from langchain_core.messages import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt):\n",
    "    ''' Get a completion from the Gemini API using Langchain\n",
    "    Args:\n",
    "        prompt (str): The prompt to send to the API\n",
    "\n",
    "    Returns:\n",
    "        str: The completion text\n",
    "    '''\n",
    "    messages = [HumanMessage(content=prompt)]\n",
    "    response = llm.invoke(messages)\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptTemplate:\n",
    "    ''' A class to represent a template for generating prompts with variables\n",
    "    Attributes:\n",
    "        template (str): The template string with variables\n",
    "        input_variables (list): A list of the variable names in the template\n",
    "    '''\n",
    "    def __init__(self, template, input_variables):\n",
    "        self.template = Template(template)\n",
    "        self.input_variables = input_variables\n",
    "    \n",
    "    def format(self, **kwargs):\n",
    "        return self.template.render(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_template = PromptTemplate(\n",
    "    template=\"Provide a brief explanation of {{ topic }}.\",\n",
    "    input_variables=[\"topic\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Simple Template Result:\")\n",
    "prompt = simple_template.format(topic=\"photosynthesis\")\n",
    "print(get_completion(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complex_template = PromptTemplate(\n",
    "    template=\"Explain the concept of {{ concept }} in the field of {{ field }} to a {{ audience }} audience, concisely.\",\n",
    "    input_variables=[\"concept\", \"field\", \"audience\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Complex Template Result:\")\n",
    "prompt = complex_template.format(\n",
    "    concept=\"neural networks\",\n",
    "    field=\"artificial intelligence\",\n",
    "    audience=\"beginner\"\n",
    ")\n",
    "print(get_completion(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditional_template = PromptTemplate(\n",
    "    template=\"My name is {{ name }} and I am {{ age }} years old. \"\n",
    "              \"{% if profession %}I work as a {{ profession }}.{% else %}I am currently not employed.{% endif %} \"\n",
    "              \"Can you give me career advice based on this information? answer concisely.\",\n",
    "    input_variables=[\"name\", \"age\", \"profession\"]\n",
    ")\n",
    "\n",
    "# Using the conditional template\n",
    "print(\"Conditional Template Result (with profession):\")\n",
    "prompt = conditional_template.format(\n",
    "    name=\"Alex\",\n",
    "    age=\"28\",\n",
    "    profession=\"software developer\"\n",
    ")\n",
    "print(get_completion(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nConditional Template Result (without profession):\")\n",
    "prompt = conditional_template.format(\n",
    "    name=\"Sam\",\n",
    "    age=\"22\",\n",
    "    profession=\"\"\n",
    ")\n",
    "print(get_completion(prompt))\n",
    "\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_format_template = PromptTemplate(\n",
    "    template=\"Analyze the following list of items:\\n\"\n",
    "              \"{% for item in items.split(',') %}\"\n",
    "              \"- {{ item.strip() }}\\n\"\n",
    "              \"{% endfor %}\"\n",
    "              \"\\nProvide a summary of the list and suggest any patterns or groupings.\",\n",
    "    input_variables=[\"items\"]\n",
    ")\n",
    "\n",
    "\n",
    "# Using the formatted list template\n",
    "print(\"Formatted List Template Result:\")\n",
    "prompt = list_format_template.format(\n",
    "    items=\"Python, JavaScript, HTML, CSS, React, Django, Flask, Node.js\"\n",
    ")\n",
    "print(get_completion(prompt))\n",
    "\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sgai-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
